{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Concepts and Principles\n",
    "## Software Defect Detection\n",
    "\n",
    "> Lazaros Panitsidis & Konstantinos Kravaritis<br />\n",
    "> MSc Data Science <br />\n",
    "> International Hellenic University <br />\n",
    "> lpanitsidis@ihu.edu.gr & kkravaritis@ihu.edu.gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Useful Python Libraries](#0)\n",
    "1. [Data Content](#1)\n",
    "1. [Feature Engineering](#2)\n",
    "     1. [Data Preprocessing](#3)\n",
    "     1. [Visualization & Analysis](#4)\n",
    "1. [Feature Selection and Random Forest Classification](#5)\n",
    "     1. [Feature Selection by Correlation](#6)\n",
    "     1. [Univariate feature selection (SelectKbest)](#7)\n",
    "     1. [Recursive Feature Elimination (RFE)](#8)\n",
    "     1. [Recursive Feature Elimination with Cross-Validation (RFECV)](#9)\n",
    "     1. [Feature importances with a forest of trees](#10)\n",
    "     1. [XGBoost Feature Importances](#11)\n",
    "     1. [Minimum Redundancy & Maximum Relevance](#12)\n",
    "1. [Feature extraction with PCA](#11)\n",
    "1. [Summary](#12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Useful Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write all the pip commands to download the packages below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import statistics as stats # https://docs.python.org/3/library/statistics.html#statistics.fmean\n",
    "#import scipy.stats as spstats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#import warnings library\n",
    "import warnings\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "with warnings.catch_warnings():\n",
    "    # Catch and ignore ConvergenceWarnings\n",
    "    warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "### Validation & Normalization methods ###\n",
    "from sklearn.model_selection import cross_validate, cross_val_score , GridSearchCV , StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "### ML models ###\n",
    "from sklearn.linear_model import LogisticRegression # C1\n",
    "from sklearn.linear_model import SGDClassifier # C1 loss: log_loss => LogisticRegression with SGD\n",
    "from sklearn.linear_model import Perceptron # C2\n",
    "from sklearn.svm import SVC # C3\n",
    "from sklearn.tree import DecisionTreeClassifier # C4\n",
    "from sklearn.ensemble import RandomForestClassifier # C5\n",
    "from sklearn.neural_network import MLPClassifier # C6\n",
    "\n",
    "### Metrics ###\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, make_scorer, classification_report\n",
    "from imblearn.metrics import geometric_mean_score # https://imbalanced-learn.org/stable/references/generated/imblearn.metrics.geometric_mean_score.html\n",
    "import time\n",
    "import timeit # https://stackoverflow.com/questions/17579357/time-time-vs-timeit-timeit\n",
    "\n",
    "\n",
    "### Pipeline ###\n",
    "from sklearn.pipeline import make_pipeline , Pipeline # https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "\n",
    "### Custom Modules ###\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from functions.data_types import optimize_dtypes\n",
    "from functions.dataframe_actions import df_info, df_clean\n",
    "from functions.ml_training import train_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the .csv files and make dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read .csv files from another directory\n",
    "data_location = \"\" # /<path>\n",
    "\n",
    "jm1 = pd.read_csv(data_location + \"jm1.csv\")\n",
    "mc1 = pd.read_csv(data_location + \"mc1.csv\")\n",
    "pc3 = pd.read_csv(data_location + \"pc3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the dataframes from non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that cointain non numeric valeus\n",
    "jm1 = df_clean(jm1)\n",
    "mc1 = df_clean(mc1)\n",
    "pc3 = df_clean(pc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract useful information about the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- information for  jm1  -----\n",
      "jm1  :  (10880, 22) (rows, columns)\n",
      "jm1  :  0 missing values\n",
      "jm1  :  1973 duplicated values\n",
      "jm1  : Value counts for  defects\n",
      "defects\n",
      "False    8777\n",
      "True     2103\n",
      "Name: count, dtype: int64\n",
      "----- information for  mc1  -----\n",
      "mc1  :  (9466, 39) (rows, columns)\n",
      "mc1  :  0 missing values\n",
      "mc1  :  7450 duplicated values\n",
      "mc1  : Value counts for  c\n",
      "c\n",
      "False    9398\n",
      "True       68\n",
      "Name: count, dtype: int64\n",
      "----- information for  pc3  -----\n",
      "pc3  :  (1563, 38) (rows, columns)\n",
      "pc3  :  0 missing values\n",
      "pc3  :  124 duplicated values\n",
      "pc3  : Value counts for  c\n",
      "c\n",
      "False    1403\n",
      "True      160\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataframes = [jm1, mc1, pc3]\n",
    "dataframe_names = [\"jm1\", \"mc1\", \"pc3\"]\n",
    "df_info(dataframes, dataframe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use map instead of LabelEncoder() to ensure that False is 0 and True is 1 in all dataframes.\n",
    "* With LabelEncoder() it depends on the order that the labels appear in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_le = LabelEncoder()\n",
    "# jm1['defects'] = class_le.fit_transform(jm1['defects'].values)\n",
    "# print(\"Classes of Label Encoder:\", class_le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the \"size\" ordinal feature to an integer value\n",
    "map_lexicon = {False: 0, True: 1}\n",
    "jm1['defects'] = jm1['defects'].map(map_lexicon)\n",
    "mc1['c'] = mc1['c'].map(map_lexicon)\n",
    "pc3['c'] = pc3['c'].map(map_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find optimal data types for faster computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jm1 = optimize_dtypes(jm1)\n",
    "mc1 = optimize_dtypes(mc1)\n",
    "pc3 = optimize_dtypes(pc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Optimal Data Types ----------\n",
      "loc                  float16\n",
      "v(g)                 float16\n",
      "ev(g)                float16\n",
      "iv(g)                float16\n",
      "n                    float16\n",
      "v                    float32\n",
      "l                    float16\n",
      "d                    float16\n",
      "i                    float16\n",
      "e                    float32\n",
      "b                    float16\n",
      "t                    float32\n",
      "lOCode                uint16\n",
      "lOComment             uint16\n",
      "lOBlank               uint16\n",
      "locCodeAndComment      uint8\n",
      "uniq_Op              float16\n",
      "uniq_Opnd            float16\n",
      "total_Op             float16\n",
      "total_Opnd           float16\n",
      "branchCount          float16\n",
      "defects                uint8\n",
      "dtype: object\n",
      "---------- Optimal Data Types ----------\n",
      "LOC_BLANK                            uint8\n",
      "BRANCH_COUNT                        uint16\n",
      "CALL_PAIRS                           uint8\n",
      "LOC_CODE_AND_COMMENT                 uint8\n",
      "LOC_COMMENTS                         uint8\n",
      "CONDITION_COUNT                     uint16\n",
      "CYCLOMATIC_COMPLEXITY                uint8\n",
      "CYCLOMATIC_DENSITY                 float16\n",
      "DECISION_COUNT                      uint16\n",
      "DESIGN_COMPLEXITY                    uint8\n",
      "DESIGN_DENSITY                       uint8\n",
      "EDGE_COUNT                          uint16\n",
      "ESSENTIAL_COMPLEXITY                 uint8\n",
      "ESSENTIAL_DENSITY                    uint8\n",
      "LOC_EXECUTABLE                      uint16\n",
      "PARAMETER_COUNT                      uint8\n",
      "GLOBAL_DATA_COMPLEXITY               uint8\n",
      "GLOBAL_DATA_DENSITY                  uint8\n",
      "HALSTEAD_CONTENT                   float16\n",
      "HALSTEAD_DIFFICULTY                float16\n",
      "HALSTEAD_EFFORT                    float32\n",
      "HALSTEAD_ERROR_EST                 float16\n",
      "HALSTEAD_LENGTH                     uint16\n",
      "HALSTEAD_LEVEL                     float16\n",
      "HALSTEAD_PROG_TIME                 float32\n",
      "HALSTEAD_VOLUME                    float16\n",
      "MAINTENANCE_SEVERITY                 uint8\n",
      "MODIFIED_CONDITION_COUNT             uint8\n",
      "MULTIPLE_CONDITION_COUNT            uint16\n",
      "NODE_COUNT                          uint16\n",
      "NORMALIZED_CYLOMATIC_COMPLEXITY    float16\n",
      "NUM_OPERANDS                        uint16\n",
      "NUM_OPERATORS                       uint16\n",
      "NUM_UNIQUE_OPERANDS                  uint8\n",
      "NUM_UNIQUE_OPERATORS                 uint8\n",
      "NUMBER_OF_LINES                     uint16\n",
      "PERCENT_COMMENTS                   float16\n",
      "LOC_TOTAL                           uint16\n",
      "c                                    uint8\n",
      "dtype: object\n",
      "---------- Optimal Data Types ----------\n",
      "LOC_BLANK                            uint8\n",
      "BRANCH_COUNT                        uint16\n",
      "CALL_PAIRS                           uint8\n",
      "LOC_CODE_AND_COMMENT                 uint8\n",
      "LOC_COMMENTS                         uint8\n",
      "CONDITION_COUNT                     uint16\n",
      "CYCLOMATIC_COMPLEXITY               uint16\n",
      "CYCLOMATIC_DENSITY                 float16\n",
      "DECISION_COUNT                      uint16\n",
      "DECISION_DENSITY                   float16\n",
      "DESIGN_COMPLEXITY                    uint8\n",
      "DESIGN_DENSITY                     float16\n",
      "EDGE_COUNT                          uint16\n",
      "ESSENTIAL_COMPLEXITY                 uint8\n",
      "ESSENTIAL_DENSITY                  float16\n",
      "LOC_EXECUTABLE                      uint16\n",
      "PARAMETER_COUNT                      uint8\n",
      "HALSTEAD_CONTENT                   float16\n",
      "HALSTEAD_DIFFICULTY                float16\n",
      "HALSTEAD_EFFORT                    float32\n",
      "HALSTEAD_ERROR_EST                 float16\n",
      "HALSTEAD_LENGTH                     uint16\n",
      "HALSTEAD_LEVEL                     float16\n",
      "HALSTEAD_PROG_TIME                 float32\n",
      "HALSTEAD_VOLUME                    float32\n",
      "MAINTENANCE_SEVERITY               float16\n",
      "MODIFIED_CONDITION_COUNT            uint16\n",
      "MULTIPLE_CONDITION_COUNT            uint16\n",
      "NODE_COUNT                          uint16\n",
      "NORMALIZED_CYLOMATIC_COMPLEXITY    float16\n",
      "NUM_OPERANDS                        uint16\n",
      "NUM_OPERATORS                       uint16\n",
      "NUM_UNIQUE_OPERANDS                 uint16\n",
      "NUM_UNIQUE_OPERATORS                 uint8\n",
      "NUMBER_OF_LINES                     uint16\n",
      "PERCENT_COMMENTS                   float16\n",
      "LOC_TOTAL                           uint16\n",
      "c                                    uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"---------- Optimal Data Types ----------\")\n",
    "print(jm1.dtypes)\n",
    "print(\"---------- Optimal Data Types ----------\")\n",
    "print(mc1.dtypes)\n",
    "print(\"---------- Optimal Data Types ----------\")\n",
    "print(pc3.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Feature variables (inputs or predictors) and Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jm1_y = jm1.defects\n",
    "jm1_x = jm1.drop('defects',axis = 1 )\n",
    "\n",
    "mc1_y = mc1.c\n",
    "mc1_x = mc1.drop('c',axis = 1 )\n",
    "\n",
    "pc3_y = pc3.c\n",
    "pc3_x = pc3.drop('c',axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "perc = Perceptron()\n",
    "linear_svm = SVC(kernel='linear')\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "tree = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "classifiers = [lr, perc, linear_svm, rbf_svm, tree, rf, mlp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple metrics\n",
    "scoring = {'Accuracy': make_scorer(accuracy_score),\n",
    "           'F1-score': make_scorer(f1_score, average='weighted'),\n",
    "           'G-Mean score': make_scorer(geometric_mean_score, average='weighted')\n",
    "          }\n",
    "\n",
    "# time: start - end time or %timeit\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "\n",
    "# sum(['fit_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "## If we use MinMaxScaler or StandardScaler, the feature names will be lost, so we do it mannually.\n",
    "\n",
    "# x_scaled = (x - x.min(axis=0)) / (x.max(axis=0)-x.min(axis=0))\n",
    "# x_scaled = (x - x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the Cross Validation folds method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rng = np.random.RandomState(13) # random number generator , use it in every random state if shuffle=True for different results.Usefull to test a specific algorithm multiple times within a for loop.\n",
    "cv=StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "#search_cv = StratifiedKFold(n_splits=3, shuffle=False, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N1: No Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazaros/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazaros/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/lazaros/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "jm1_nn_results = train_classifiers(classifiers, jm1_x, jm1_y, cv, scoring)\n",
    "jm1_nn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jm1_nn_results = pd.DataFrame.from_dict(jm1_nn_results, orient='index')\n",
    "df_jm1_nn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc1_nn_results = train_classifiers(classifiers, mc1_x, mc1_y, cv, scoring)\n",
    "mc1_nn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc3_nn_results = train_classifiers(classifiers, pc3_x, pc3_y, cv, scoring)\n",
    "pc3_nn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N2: Min-Max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jm1_mmn_results = train_classifiers(classifiers, jm1_x, jm1_y, cv, scoring, min_max_scaler)\n",
    "jm1_mmn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc1_mmn_results = train_classifiers(classifiers, mc1_x, mc1_y, cv, scoring, min_max_scaler)\n",
    "mc1_mmn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc3_mmn_results = train_classifiers(classifiers, pc3_x, pc3_y, cv, scoring, min_max_scaler)\n",
    "pc3_mmn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N3: Feature Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jm1_fs_results = train_classifiers(classifiers, jm1_x, jm1_y, cv, scoring, std_scaler)\n",
    "jm1_fs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc1_fs_results = train_classifiers(classifiers, mc1_x, mc1_y, cv, scoring, std_scaler)\n",
    "mc1_fs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc3_fs_results = train_classifiers(classifiers, pc3_x, pc3_y, cv, scoring, std_scaler)\n",
    "pc3_fs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## example without using train_classifiers function\n",
    "\n",
    "# lr_fs_pipe = Pipeline([('std_scaler', StandardScaler()), ('lr', LogisticRegression())])\n",
    "\n",
    "# jm1_lr_fs_scores = cross_validate(lr_fs_pipe, jm1_x, jm1_y,\n",
    "#                         cv=cv, scoring=scoring,\n",
    "#                         n_jobs=None, return_train_score=False)\n",
    "\n",
    "# jm1_lr_fs_accuracy = stats.fmean(jm1_lr_fs_scores['test_Accuracy'])\n",
    "# jm1_lr_fs_f1 = stats.fmean(jm1_lr_fs_scores['test_F1-score'])\n",
    "# jm1_lr_fs_g_mean = stats.fmean(jm1_lr_fs_scores['test_G-Mean score'])\n",
    "# jm1_lr_fs_fit_time = sum(jm1_lr_fs_scores['fit_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ΕΡΩΤΉΣΕΙΣ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RepeatedStratifiedKFold or StratifiedKFold\n",
    "\n",
    "default or tuning models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
